{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../.')\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from utils_plotting import *\n",
    "from Data.DataGenerator import *\n",
    "from Models.Models_normal import *\n",
    "import torch.optim as optim\n",
    "from Training.Analysis import fixed_model_batch_analysis\n",
    "from Data.DataLoader import *\n",
    "from Training.Spike_loss import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, SpectralClustering, AgglomerativeClustering, BisectingKMeans\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.linear_model import RANSACRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader Batch Shapes:\n",
      "Batch 1: Images Shape = torch.Size([64, 1, 28, 28]), Labels Shape = torch.Size([64])\n",
      "\n",
      "Validation Loader Batch Shapes:\n",
      "Batch 1: Images Shape = torch.Size([64, 1, 28, 28]), Labels Shape = torch.Size([64])\n",
      "\n",
      "Test Loader Batch Shapes:\n",
      "Batch 1: Images Shape = torch.Size([64, 1, 28, 28]), Labels Shape = torch.Size([64])\n",
      "\n",
      "Train Samples Shape: torch.Size([50000, 784])\n",
      "Train Labels Shape: torch.Size([50000])\n",
      "\n",
      "Validation Samples Shape: torch.Size([10000, 784])\n",
      "Validation Labels Shape: torch.Size([10000])\n",
      "\n",
      "Test Samples Shape: torch.Size([10000, 784])\n",
      "Test Labels Shape: torch.Size([10000])\n",
      "\n",
      "Train Label Frequencies: Counter({1: 5678, 7: 5175, 3: 5101, 9: 4988, 2: 4968, 6: 4951, 0: 4932, 4: 4859, 8: 4842, 5: 4506})\n",
      "Validation Label Frequencies: Counter({7: 1090, 1: 1064, 3: 1030, 8: 1009, 0: 991, 2: 990, 4: 983, 6: 967, 9: 961, 5: 915})\n",
      "Test Label Frequencies: Counter({1: 1135, 2: 1032, 7: 1028, 3: 1010, 9: 1009, 4: 982, 0: 980, 8: 974, 6: 958, 5: 892})\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "arch = (784, [256, 128, 128, 128, 64, 64, 64, 64, 64, 64, 32, 10])\n",
    "\n",
    "model = MNIST_classifier(n_in=arch[0], layer_list=arch[1], bias=0)\n",
    "# model = MNIST_classifier(n_in=784, layer_list=[128, 128, 128, 64, 64, 64, 32, 16, 8, 3], bias=0)\n",
    "# model = MNIST_classifier(n_in=784, layer_list=[128, 128, 128, 64, 64, 64, 64, 64, 64, 32, 10], bias=0)\n",
    "\n",
    "state_dict = torch.load('/home/mila/m/mehrab.hamidi/scratch/training_res/november_res/mnist/normal/bias_0.0001/mnist_training/try_num2/epoch_120/model.pt', weights_only=False)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# load the data\n",
    "_, _, _, train_samples, train_labels, val_samples, val_labels, test_samples, test_labels = get_mnist_data_loaders()\n",
    "\n",
    "\n",
    "\n",
    "dataset_target_samples = train_samples\n",
    "dataset_target_labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    }
   ],
   "source": [
    "anal_path = '../../spike_analysis/spikes_hamming_new25/'\n",
    "if not os.path.isdir(anal_path):\n",
    "    os.makedirs(anal_path)\n",
    "results_dict = fixed_model_batch_analysis(model, dataset_target_samples, dataset_target_labels, device, '{}_{}'.format(anal_path, 'val_'), 'analyze', plotting=False)\n",
    "plt.close()\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation pattern analysis\n",
    "def analyze_activation_patterns(relu_outputs):\n",
    "    binary_matrices = [(layer_output > 0).int() for layer_output in relu_outputs]\n",
    "    activation_patterns = torch.cat(binary_matrices, dim=1)  # Shape: (num_data_points, total_neurons)\n",
    "    unique_patterns, counts = torch.unique(activation_patterns, dim=0, return_counts=True)\n",
    "    num_unique_patterns = unique_patterns.size(0)\n",
    "    hamming_distances = torch.cdist(unique_patterns.float(), unique_patterns.float(), p=0).cpu().numpy()\n",
    "    hamming_distances_flat = hamming_distances[np.triu_indices(len(unique_patterns), k=1)]\n",
    "    return unique_patterns, counts, hamming_distances_flat\n",
    "\n",
    "# Class and spike analysis\n",
    "def analyze_all_classes_and_spikes(model, mnist_data, mnist_labels, device, anal_path):\n",
    "    num_classes = len(np.unique(mnist_labels))\n",
    "    all_classes_hamming_distances = []\n",
    "    all_spikes_hamming_distances = {}\n",
    "\n",
    "    for class_idx in tqdm(range(num_classes)):\n",
    "        # Filter data for the current class\n",
    "        class_data_indices = np.where(mnist_labels == class_idx)[0]\n",
    "        results_dict_class = fixed_model_batch_analysis(\n",
    "            model, torch.Tensor(mnist_data[class_data_indices]), mnist_labels[class_data_indices], device,\n",
    "            '{}_{}'.format(anal_path, 'train_'), 'analyze', plotting=False\n",
    "        )\n",
    "        relu_outputs_class = results_dict_class['representations']\n",
    "        _, _, class_hamming_distances = analyze_activation_patterns(relu_outputs_class[1:])\n",
    "        all_classes_hamming_distances.append(class_hamming_distances)\n",
    "\n",
    "        # Detect spikes (hyperplanes) in the class data\n",
    "        points = torch.Tensor(np.array(results_dict_class['pca_2'][7]).transpose())\n",
    "        detected_hyperplanes, total_error, assigned_points = spike_detection_nd(points)\n",
    "        assignments, _, _ = assign_points_to_hyperplanes(points, detected_hyperplanes)\n",
    "\n",
    "        all_spikes_hamming_distances[class_idx] = []\n",
    "\n",
    "        # Analyze each spike\n",
    "        sorted_hyperplanes_indices = np.argsort([np.sum(assignments.numpy() == i) for i in range(len(detected_hyperplanes))])[::-1]\n",
    "        for spike_idx in sorted_hyperplanes_indices:\n",
    "            spike_data_indices = class_data_indices[assignments == spike_idx]\n",
    "            results_dict_spike = fixed_model_batch_analysis(\n",
    "                model, torch.Tensor(mnist_data[spike_data_indices]), mnist_labels[spike_data_indices], device,\n",
    "                '{}_{}'.format(anal_path, 'train_'), 'analyze', plotting=False\n",
    "            )\n",
    "            relu_outputs_spike = results_dict_spike['representations']\n",
    "            _, _, spike_hamming_distances = analyze_activation_patterns(relu_outputs_spike[1:])\n",
    "            all_spikes_hamming_distances[class_idx].append(spike_hamming_distances)\n",
    "\n",
    "    return all_classes_hamming_distances, all_spikes_hamming_distances\n",
    "\n",
    "def plot_hamming_distance_analysis(all_classes_hamming_distances, all_spikes_hamming_distances, anal_path):\n",
    "    num_classes = len(all_classes_hamming_distances)\n",
    "    fig = plt.figure(figsize=(36, 50))\n",
    "    outer_grid = fig.add_gridspec(num_classes, 11, width_ratios=[1, 1] + [1] * 9)  # Adjust grid size if necessary\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        # Class hamming distances\n",
    "        class_box_ax = fig.add_subplot(outer_grid[class_idx, 1])\n",
    "        class_box_ax.boxplot(all_classes_hamming_distances[class_idx])\n",
    "        class_box_ax.set_title(f'Class {class_idx}')\n",
    "        class_box_ax.set_ylabel('Hamming Distance')\n",
    "\n",
    "        class_mean = np.mean(all_classes_hamming_distances[class_idx])\n",
    "\n",
    "        # Spike hamming distances\n",
    "        max_spikes_to_plot = 9  # Adjust based on the grid size\n",
    "        for spike_idx, spike_hamming_distances in enumerate(all_spikes_hamming_distances[class_idx][:max_spikes_to_plot]):\n",
    "            spike_box_ax = fig.add_subplot(outer_grid[class_idx, spike_idx + 2])\n",
    "            spike_box_ax.boxplot(spike_hamming_distances)\n",
    "            spike_box_ax.axhline(class_mean, color='gray', linestyle='--', alpha=0.7)\n",
    "            spike_box_ax.set_title(f'Spike {spike_idx}')\n",
    "            spike_box_ax.set_xlabel('Spike')\n",
    "            spike_box_ax.set_ylabel('Hamming Distance')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{anal_path}/boxplot_all_layers.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Final Integration\n",
    "def run_analysis(model, dataset_target_samples, dataset_target_labels, device, anal_path):\n",
    "    mnist_labels = dataset_target_labels.detach().cpu().numpy()\n",
    "    mnist_data = dataset_target_samples.detach().cpu().numpy().reshape(dataset_target_samples.shape[0], 28 * 28)\n",
    "    \n",
    "    all_classes_hamming_distances, all_spikes_hamming_distances = analyze_all_classes_and_spikes(\n",
    "        model, mnist_data, mnist_labels, device, anal_path\n",
    "    )\n",
    "\n",
    "    return all_classes_hamming_distances, all_spikes_hamming_distances\n",
    "\n",
    "\n",
    "def plot_hamming_distance_analysis_with_scatter(all_classes_hamming_distances, all_spikes_hamming_distances, results_dict, mnist_labels, anal_path):\n",
    "    num_classes = len(all_classes_hamming_distances)\n",
    "    fig = plt.figure(figsize=(50, 60))\n",
    "    outer_grid = fig.add_gridspec(num_classes, 11, width_ratios=[2, 1] + [1] * 9)  # First column is wider for scatter plots\n",
    "\n",
    "    for class_idx in tqdm(range(num_classes)):\n",
    "        # Extract class data and detect spikes (hyperplanes)\n",
    "        class_data_indices = np.where(mnist_labels == class_idx)[0]\n",
    "        points = torch.Tensor(np.array(results_dict['pca_2'][7]).transpose())\n",
    "        detected_hyperplanes, total_error, assigned_points = spike_detection_nd(points)\n",
    "        assignments, _, _ = assign_points_to_hyperplanes(points, detected_hyperplanes)\n",
    "\n",
    "        # Scatter plot for the class points and spikes\n",
    "        scatter_ax = fig.add_subplot(outer_grid[class_idx, 0])\n",
    "        colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
    "        for spike_idx in range(len(detected_hyperplanes)):\n",
    "            scatter_ax.scatter(\n",
    "                points[assignments == spike_idx, 0],\n",
    "                points[assignments == spike_idx, 1],\n",
    "                s=50,\n",
    "                color=colors(spike_idx),\n",
    "                alpha=0.5,\n",
    "                label=f'Spike {spike_idx}'\n",
    "            )\n",
    "        for coef, intercept, _, _ in detected_hyperplanes:\n",
    "            xx = np.linspace(points[:, 0].min(), points[:, 0].max(), 100)\n",
    "            yy = (-coef[0] * xx - intercept) / coef[1]\n",
    "            scatter_ax.plot(xx, yy, color='gray', linestyle='--', alpha=0.5)\n",
    "        scatter_ax.set_title(f'Class {class_idx} Scatter Plot')\n",
    "        scatter_ax.set_xlabel('PCA Component 1')\n",
    "        scatter_ax.set_ylabel('PCA Component 2')\n",
    "        scatter_ax.legend()\n",
    "\n",
    "        # Histogram for class hamming distances\n",
    "        class_hist_ax = fig.add_subplot(outer_grid[class_idx, 1])\n",
    "        class_hist_ax.hist(\n",
    "            all_classes_hamming_distances[class_idx],\n",
    "            bins=20,\n",
    "            color='blue',\n",
    "            alpha=0.7,\n",
    "            weights=np.ones(len(all_classes_hamming_distances[class_idx])) / len(all_classes_hamming_distances[class_idx]) * 100,\n",
    "        )\n",
    "        class_hist_ax.set_title(f'Class {class_idx} Hamming Distance Histogram')\n",
    "        class_hist_ax.set_xlabel('Hamming Distance')\n",
    "        class_hist_ax.set_ylabel('Percentage (%)')\n",
    "\n",
    "        # Spike histograms\n",
    "        max_spikes_to_plot = 9  # Limit the number of spikes visualized\n",
    "        for spike_idx, spike_hamming_distances in enumerate(all_spikes_hamming_distances[class_idx][:max_spikes_to_plot]):\n",
    "            spike_hist_ax = fig.add_subplot(outer_grid[class_idx, spike_idx + 2])\n",
    "            spike_hist_ax.hist(\n",
    "                spike_hamming_distances,\n",
    "                bins=20,\n",
    "                color='red',\n",
    "                alpha=0.7,\n",
    "                weights=np.ones(len(spike_hamming_distances)) / len(spike_hamming_distances) * 100,\n",
    "            )\n",
    "            spike_hist_ax.set_title(f'Spike {spike_idx} Hamming Distance Histogram')\n",
    "            spike_hist_ax.set_xlabel('Hamming Distance')\n",
    "            spike_hist_ax.set_ylabel('Percentage (%)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{anal_path}/hamming_distance_with_scatter.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes_hamming_distances, all_spikes_hamming_distances = run_analysis(model, dataset_target_samples, dataset_target_labels, device, anal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hamming_distance_analysis(all_classes_hamming_distances, all_spikes_hamming_distances, anal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n",
      "/tmp/ipykernel_1303068/175616107.py:104: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colors = plt.cm.get_cmap('tab10', len(detected_hyperplanes))\n"
     ]
    }
   ],
   "source": [
    "plot_hamming_distance_analysis_with_scatter(all_classes_hamming_distances, all_spikes_hamming_distances, results_dict, dataset_target_labels, anal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [17:00<00:00, 85.07s/it]\n"
     ]
    }
   ],
   "source": [
    "mnist_labels =  dataset_target_labels.detach().cpu().numpy()\n",
    "mnist_data = dataset_target_samples.detach().cpu().numpy().reshape(dataset_target_samples.shape[0], 28 * 28)\n",
    "# Number of classes\n",
    "num_classes = len(np.unique(mnist_labels))\n",
    "key = 'representations'\n",
    "results_dict['representations']\n",
    "\n",
    "for idx_layer in tqdm(range(len(results_dict[key]))):\n",
    "\n",
    "    # mnist_target_data = np.array(results_dict[key][idx_layer]).transpose()\n",
    "    mnist_target_data = results_dict[key][idx_layer].detach().cpu().numpy()\n",
    "\n",
    "    plot_target_data = np.array(results_dict['pca_2'][idx_layer]).transpose()\n",
    "\n",
    "    # Create the overall figure\n",
    "    fig = plt.figure(figsize=(36, 50))\n",
    "    outer_grid = fig.add_gridspec(num_classes + 1, 2, width_ratios=[1.2, 1.8], height_ratios=[1] + [0.6] * num_classes)\n",
    "\n",
    "\n",
    "    # Original PCA plot for the entire dataset\n",
    "    ax_pca = fig.add_subplot(outer_grid[0, 0])\n",
    "    ax_pca.set_title('2D PCA of Original MNIST Dataset')\n",
    "\n",
    "    colors = np.array(plot_data_projection(ax_pca, idx_layer, results_dict['pca_2'], labels_all=results_dict['labels']))\n",
    "\n",
    "\n",
    "    # Process each class for clustering and sample visualization\n",
    "    for class_idx in range(num_classes):\n",
    "        # Extract the data points belonging to the current class\n",
    "        class_data_indices = np.where(mnist_labels == class_idx)[0]\n",
    "        class_data = mnist_data[class_data_indices]\n",
    "        mnist_class_data = mnist_target_data[np.where(mnist_labels == class_idx)[0]]\n",
    "\n",
    "        plot_class_data = plot_target_data[np.where(mnist_labels == class_idx)[0]]\n",
    "\n",
    "        points = class_data.copy()\n",
    "\n",
    "        # Apply iterative line fitting clustering to the class data\n",
    "        # clusters, cluster_indices = iterative_line_fitting(class_data)\n",
    "        detected_hyperplanes, total_error, assigned_points = spike_detection_nd(points)\n",
    "        if len(detected_hyperplanes) == 0:\n",
    "            continue\n",
    "        assignments, _, _ = assign_points_to_hyperplanes(torch.Tensor(points), detected_hyperplanes)\n",
    "\n",
    "        # Plot the clustering for the current class\n",
    "        class_cluster_ax = fig.add_subplot(outer_grid[class_idx + 1, 0])\n",
    "        class_cluster_ax.set_title(f'Clustering of Class {class_idx} (PCA Reduced)')\n",
    "        for cluster_idx in range(len(detected_hyperplanes)):\n",
    "            cluster_color = colors[cluster_idx % len(colors)]\n",
    "            class_cluster_ax.scatter(plot_class_data[assignments == cluster_idx, 0], plot_class_data[assignments == cluster_idx, 1], label=f'Spike {cluster_idx + 1}', color=cluster_color)\n",
    "        class_cluster_ax.set_xlabel('First Principal Component')\n",
    "        class_cluster_ax.set_ylabel('Second Principal Component')\n",
    "        class_cluster_ax.legend(title='Clusters')\n",
    "\n",
    "        # Create a grid to hold the sample images for each cluster of the current class\n",
    "        # sample_grid = fig.add_subplot(outer_grid[class_idx + 1, 1])\n",
    "        cluster_gridspec = gridspec.GridSpecFromSubplotSpec(1, len(detected_hyperplanes), subplot_spec=outer_grid[class_idx + 1, 1], wspace=0.3)\n",
    "\n",
    "        for cluster_idx in range(len(detected_hyperplanes)):\n",
    "            cluster = class_data[assignments == cluster_idx]\n",
    "            # Create a subplot for each cluster to contain its samples\n",
    "            cluster_ax = fig.add_subplot(cluster_gridspec[0, cluster_idx])\n",
    "            cluster_ax.axis('off')\n",
    "            cluster_ax.set_title(f'Cluster {cluster_idx + 1}', fontsize=10)\n",
    "\n",
    "            # Plot 15 random samples for the current cluster in a 3x5 grid\n",
    "            num_samples = min(60, len(cluster))\n",
    "            rows, cols = 6, 10\n",
    "            grid = gridspec.GridSpecFromSubplotSpec(rows, cols, subplot_spec=cluster_gridspec[0, cluster_idx], wspace=0.1, hspace=0.1)\n",
    "\n",
    "            for sample_idx in range(num_samples):\n",
    "                random_idx = np.random.choice(np.where(assignments == cluster_idx)[0])\n",
    "                sample_image = class_data[random_idx].reshape(28, 28)\n",
    "                \n",
    "                # Determine position within the 3x5 grid\n",
    "                row, col = divmod(sample_idx, cols)\n",
    "                sub_ax = fig.add_subplot(grid[row, col])\n",
    "                sub_ax.imshow(sample_image, cmap='gray')\n",
    "                sub_ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)  # Add spacing to avoid overlap\n",
    "    fig.savefig(f\"{anal_path}spike_plot{idx_layer}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
